{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215667d0-d8cd-49cb-9c71-4a4ce6f48c24",
   "metadata": {},
   "source": [
    "# IMPORTS AND CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95321a41-b85c-4239-84ac-e8541946ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully\n",
      "=== FRUITS & VEGETABLES QUALITY ASSESSMENT SYSTEM ===\n",
      "\n",
      "ðŸ“ Dataset location: C:\\Users\\bbuser\\Desktop\\FRUITS & VEGETABLES\n",
      "\n",
      "âœ… Database initialized successfully\n",
      "Step 0: Checking dataset structure...\n",
      "ðŸ” Checking dataset structure...\n",
      "âœ… Found: fruits\n",
      "  âœ… Found: apple\n",
      "    âœ… ripe: 117 images\n",
      "    âœ… unripe: 5 images\n",
      "    âœ… overripe: 96 images\n",
      "    âœ… bruised: 59 images\n",
      "  âœ… Found: banana\n",
      "    âœ… ripe: 63 images\n",
      "    âœ… unripe: 38 images\n",
      "    âœ… overripe: 174 images\n",
      "    âœ… bruised: 37 images\n",
      "  âœ… Found: mango\n",
      "    âœ… ripe: 72 images\n",
      "    âœ… unripe: 96 images\n",
      "    âœ… overripe: 146 images\n",
      "    âœ… bruised: 90 images\n",
      "âœ… Found: vegetables\n",
      "  âœ… Found: cucumber\n",
      "    âœ… ripe: 210 images\n",
      "    âœ… unripe: 12 images\n",
      "    âœ… overripe: 110 images\n",
      "    âœ… bruised: 23 images\n",
      "  âœ… Found: tomato\n",
      "    âœ… ripe: 23 images\n",
      "    âœ… unripe: 8 images\n",
      "    âœ… overripe: 373 images\n",
      "    âœ… bruised: 6 images\n",
      "\n",
      "ðŸ“Š TOTAL IMAGES FOUND: 1758\n",
      "\n",
      "Step 1: Setting up training environment...\n",
      "ðŸ¤– No trained model found. Starting training...\n",
      "Step 1: Checking train/validation splits...\n",
      "âœ… Train/val splits already exist\n",
      "\n",
      "Step 2: Analyzing dataset balance...\n",
      "=== DATASET BALANCE ANALYSIS ===\n",
      "unripe: 125 images (8.9%)\n",
      "ripe: 386 images (27.6%)\n",
      "overripe: 717 images (51.3%)\n",
      "bruised: 170 images (12.2%)\n",
      "Total images: 1398\n",
      "\n",
      "=== IMBALANCE ANALYSIS ===\n",
      "ðŸš¨ CRITICALLY IMBALANCED: apple_unripe: 4 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: cucumber_bruised: 18 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: cucumber_unripe: 9 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_bruised: 4 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_ripe: 18 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_unripe: 6 images\n",
      "=== DATASET BALANCE ANALYSIS ===\n",
      "unripe: 34 images (9.4%)\n",
      "ripe: 99 images (27.5%)\n",
      "overripe: 182 images (50.6%)\n",
      "bruised: 45 images (12.5%)\n",
      "Total images: 360\n",
      "\n",
      "=== IMBALANCE ANALYSIS ===\n",
      "ðŸš¨ CRITICALLY IMBALANCED: apple_bruised: 12 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: apple_unripe: 1 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: banana_bruised: 8 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: banana_ripe: 13 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: banana_unripe: 8 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: cucumber_bruised: 5 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: cucumber_unripe: 3 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: mango_bruised: 18 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: mango_ripe: 15 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_bruised: 2 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_ripe: 5 images\n",
      "ðŸš¨ CRITICALLY IMBALANCED: tomato_unripe: 2 images\n",
      "\n",
      "Step 3: Calculating class weights...\n",
      "=== CLASS WEIGHTS ===\n",
      "Class unripe: 125 samples -> weight: 2.80\n",
      "Class ripe: 386 samples -> weight: 0.91\n",
      "Class overripe: 717 samples -> weight: 0.49\n",
      "Class bruised: 170 samples -> weight: 2.06\n",
      "\n",
      "Step 4: Creating dataset pipelines...\n",
      "ðŸ“ Loading dataset from: C:\\Users\\bbuser\\Desktop\\FRUITS & VEGETABLES\\train\n",
      "  âœ… apple_bruised: 47 images -> class 3\n",
      "  âœ… apple_overripe: 76 images -> class 2\n",
      "  âœ… apple_ripe: 93 images -> class 1\n",
      "  âœ… apple_unripe: 4 images -> class 0\n",
      "  âœ… banana_bruised: 29 images -> class 3\n",
      "  âœ… banana_overripe: 139 images -> class 2\n",
      "  âœ… banana_ripe: 50 images -> class 1\n",
      "  âœ… banana_unripe: 30 images -> class 0\n",
      "  âœ… cucumber_bruised: 18 images -> class 3\n",
      "  âœ… cucumber_overripe: 88 images -> class 2\n",
      "  âœ… cucumber_ripe: 168 images -> class 1\n",
      "  âœ… cucumber_unripe: 9 images -> class 0\n",
      "  âœ… mango_bruised: 72 images -> class 3\n",
      "  âœ… mango_overripe: 116 images -> class 2\n",
      "  âœ… mango_ripe: 57 images -> class 1\n",
      "  âœ… mango_unripe: 76 images -> class 0\n",
      "  âœ… tomato_bruised: 4 images -> class 3\n",
      "  âœ… tomato_overripe: 298 images -> class 2\n",
      "  âœ… tomato_ripe: 18 images -> class 1\n",
      "  âœ… tomato_unripe: 6 images -> class 0\n",
      "ðŸ“Š Total images: 1398\n",
      "ðŸ“Š Class distribution: [125 386 717 170]\n",
      "ðŸ“ Loading dataset from: C:\\Users\\bbuser\\Desktop\\FRUITS & VEGETABLES\\val\n",
      "  âœ… apple_bruised: 12 images -> class 3\n",
      "  âœ… apple_overripe: 20 images -> class 2\n",
      "  âœ… apple_ripe: 24 images -> class 1\n",
      "  âœ… apple_unripe: 1 images -> class 0\n",
      "  âœ… banana_bruised: 8 images -> class 3\n",
      "  âœ… banana_overripe: 35 images -> class 2\n",
      "  âœ… banana_ripe: 13 images -> class 1\n",
      "  âœ… banana_unripe: 8 images -> class 0\n",
      "  âœ… cucumber_bruised: 5 images -> class 3\n",
      "  âœ… cucumber_overripe: 22 images -> class 2\n",
      "  âœ… cucumber_ripe: 42 images -> class 1\n",
      "  âœ… cucumber_unripe: 3 images -> class 0\n",
      "  âœ… mango_bruised: 18 images -> class 3\n",
      "  âœ… mango_overripe: 30 images -> class 2\n",
      "  âœ… mango_ripe: 15 images -> class 1\n",
      "  âœ… mango_unripe: 20 images -> class 0\n",
      "  âœ… tomato_bruised: 2 images -> class 3\n",
      "  âœ… tomato_overripe: 75 images -> class 2\n",
      "  âœ… tomato_ripe: 5 images -> class 1\n",
      "  âœ… tomato_unripe: 2 images -> class 0\n",
      "ðŸ“Š Total images: 360\n",
      "ðŸ“Š Class distribution: [ 34  99 182  45]\n",
      "âœ… Datasets created successfully!\n",
      "\n",
      "Step 5: Building and compiling model...\n",
      "Building improved model with 4 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbuser\\AppData\\Local\\Temp\\ipykernel_2404\\2965266403.py:596: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model layers: 154\n",
      "Trainable layers: 20\n",
      "WARNING:tensorflow:From C:\\Users\\bbuser\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "âœ… Model compiled successfully!\n",
      "\n",
      "Step 6: Training model...\n",
      "Training for 30 epochs with early stopping\n",
      "Epoch 1/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 3s/step - class_out_accuracy: 0.2010 - class_out_categorical_accuracy: 0.2010 - class_out_loss: 1.9377 - freshness_scaled_loss: 12.7291 - freshness_scaled_mae: 2.9681 - freshness_scaled_mse: 12.7316 - loss: 3.2140 - val_class_out_accuracy: 0.4806 - val_class_out_categorical_accuracy: 0.4806 - val_class_out_loss: 1.4882 - val_freshness_scaled_loss: 9.4063 - val_freshness_scaled_mae: 2.6368 - val_freshness_scaled_mse: 9.1323 - val_loss: 2.3992 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 3s/step - class_out_accuracy: 0.2225 - class_out_categorical_accuracy: 0.2225 - class_out_loss: 1.8920 - freshness_scaled_loss: 12.5010 - freshness_scaled_mae: 2.9108 - freshness_scaled_mse: 12.5056 - loss: 3.1415 - val_class_out_accuracy: 0.4194 - val_class_out_categorical_accuracy: 0.4194 - val_class_out_loss: 1.4293 - val_freshness_scaled_loss: 9.6041 - val_freshness_scaled_mae: 2.6567 - val_freshness_scaled_mse: 9.3905 - val_loss: 2.3636 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 3s/step - class_out_accuracy: 0.2332 - class_out_categorical_accuracy: 0.2332 - class_out_loss: 1.7927 - freshness_scaled_loss: 11.7864 - freshness_scaled_mae: 2.8075 - freshness_scaled_mse: 11.7261 - loss: 2.9636 - val_class_out_accuracy: 0.3528 - val_class_out_categorical_accuracy: 0.3528 - val_class_out_loss: 1.4013 - val_freshness_scaled_loss: 8.7964 - val_freshness_scaled_mae: 2.5211 - val_freshness_scaled_mse: 8.6147 - val_loss: 2.2560 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 3s/step - class_out_accuracy: 0.2582 - class_out_categorical_accuracy: 0.2582 - class_out_loss: 1.7622 - freshness_scaled_loss: 10.6896 - freshness_scaled_mae: 2.6520 - freshness_scaled_mse: 10.6988 - loss: 2.8327 - val_class_out_accuracy: 0.3278 - val_class_out_categorical_accuracy: 0.3278 - val_class_out_loss: 1.3779 - val_freshness_scaled_loss: 7.7108 - val_freshness_scaled_mae: 2.3445 - val_freshness_scaled_mse: 7.5593 - val_loss: 2.1259 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - class_out_accuracy: 0.2525 - class_out_categorical_accuracy: 0.2525 - class_out_loss: 1.7349 - freshness_scaled_loss: 10.1742 - freshness_scaled_mae: 2.5331 - freshness_scaled_mse: 10.2170 - loss: 2.7579 - val_class_out_accuracy: 0.3500 - val_class_out_categorical_accuracy: 0.3500 - val_class_out_loss: 1.3640 - val_freshness_scaled_loss: 6.9028 - val_freshness_scaled_mae: 2.2153 - val_freshness_scaled_mse: 6.7584 - val_loss: 2.0319 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - class_out_accuracy: 0.2668 - class_out_categorical_accuracy: 0.2668 - class_out_loss: 1.7247 - freshness_scaled_loss: 10.0261 - freshness_scaled_mae: 2.5496 - freshness_scaled_mse: 9.9768 - loss: 2.7229 - val_class_out_accuracy: 0.3861 - val_class_out_categorical_accuracy: 0.3861 - val_class_out_loss: 1.3511 - val_freshness_scaled_loss: 6.2892 - val_freshness_scaled_mae: 2.1016 - val_freshness_scaled_mse: 6.1749 - val_loss: 1.9617 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.2825 - class_out_categorical_accuracy: 0.2825 - class_out_loss: 1.6850 - freshness_scaled_loss: 9.7253 - freshness_scaled_mae: 2.5081 - freshness_scaled_mse: 9.7307 - loss: 2.6574 - val_class_out_accuracy: 0.5167 - val_class_out_categorical_accuracy: 0.5167 - val_class_out_loss: 1.3391 - val_freshness_scaled_loss: 5.3058 - val_freshness_scaled_mae: 1.9327 - val_freshness_scaled_mse: 5.1971 - val_loss: 1.8518 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 3s/step - class_out_accuracy: 0.3190 - class_out_categorical_accuracy: 0.3190 - class_out_loss: 1.6387 - freshness_scaled_loss: 8.8402 - freshness_scaled_mae: 2.3475 - freshness_scaled_mse: 8.8385 - loss: 2.5205 - val_class_out_accuracy: 0.6083 - val_class_out_categorical_accuracy: 0.6083 - val_class_out_loss: 1.3243 - val_freshness_scaled_loss: 4.5876 - val_freshness_scaled_mae: 1.8132 - val_freshness_scaled_mse: 4.4877 - val_loss: 1.7654 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - class_out_accuracy: 0.2983 - class_out_categorical_accuracy: 0.2983 - class_out_loss: 1.6869 - freshness_scaled_loss: 8.6216 - freshness_scaled_mae: 2.3217 - freshness_scaled_mse: 8.6082 - loss: 2.5474 - val_class_out_accuracy: 0.6194 - val_class_out_categorical_accuracy: 0.6194 - val_class_out_loss: 1.3079 - val_freshness_scaled_loss: 4.0316 - val_freshness_scaled_mae: 1.7250 - val_freshness_scaled_mse: 3.9329 - val_loss: 1.6933 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - class_out_accuracy: 0.3119 - class_out_categorical_accuracy: 0.3119 - class_out_loss: 1.6322 - freshness_scaled_loss: 8.5510 - freshness_scaled_mae: 2.3057 - freshness_scaled_mse: 8.5640 - loss: 2.4894 - val_class_out_accuracy: 0.6306 - val_class_out_categorical_accuracy: 0.6306 - val_class_out_loss: 1.2825 - val_freshness_scaled_loss: 3.4917 - val_freshness_scaled_mae: 1.6303 - val_freshness_scaled_mse: 3.4037 - val_loss: 1.6140 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - class_out_accuracy: 0.3555 - class_out_categorical_accuracy: 0.3555 - class_out_loss: 1.5484 - freshness_scaled_loss: 8.3217 - freshness_scaled_mae: 2.2547 - freshness_scaled_mse: 8.3249 - loss: 2.3799 - val_class_out_accuracy: 0.6417 - val_class_out_categorical_accuracy: 0.6417 - val_class_out_loss: 1.2609 - val_freshness_scaled_loss: 3.2177 - val_freshness_scaled_mae: 1.5734 - val_freshness_scaled_mse: 3.1340 - val_loss: 1.5656 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - class_out_accuracy: 0.3648 - class_out_categorical_accuracy: 0.3648 - class_out_loss: 1.5134 - freshness_scaled_loss: 7.6270 - freshness_scaled_mae: 2.1599 - freshness_scaled_mse: 7.6212 - loss: 2.2769 - val_class_out_accuracy: 0.6611 - val_class_out_categorical_accuracy: 0.6611 - val_class_out_loss: 1.2431 - val_freshness_scaled_loss: 3.2354 - val_freshness_scaled_mae: 1.5754 - val_freshness_scaled_mse: 3.1548 - val_loss: 1.5503 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - class_out_accuracy: 0.3827 - class_out_categorical_accuracy: 0.3827 - class_out_loss: 1.5088 - freshness_scaled_loss: 6.8661 - freshness_scaled_mae: 2.0693 - freshness_scaled_mse: 6.8751 - loss: 2.1940 - val_class_out_accuracy: 0.6972 - val_class_out_categorical_accuracy: 0.6972 - val_class_out_loss: 1.2244 - val_freshness_scaled_loss: 3.2230 - val_freshness_scaled_mae: 1.5649 - val_freshness_scaled_mse: 3.1481 - val_loss: 1.5317 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.4063 - class_out_categorical_accuracy: 0.4063 - class_out_loss: 1.4780 - freshness_scaled_loss: 7.0233 - freshness_scaled_mae: 2.0633 - freshness_scaled_mse: 7.0210 - loss: 2.1803 - val_class_out_accuracy: 0.7167 - val_class_out_categorical_accuracy: 0.7167 - val_class_out_loss: 1.2077 - val_freshness_scaled_loss: 3.2176 - val_freshness_scaled_mae: 1.5611 - val_freshness_scaled_mse: 3.1460 - val_loss: 1.5161 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - class_out_accuracy: 0.3834 - class_out_categorical_accuracy: 0.3834 - class_out_loss: 1.4874 - freshness_scaled_loss: 6.8509 - freshness_scaled_mae: 2.0127 - freshness_scaled_mse: 6.8470 - loss: 2.1716 - val_class_out_accuracy: 0.7000 - val_class_out_categorical_accuracy: 0.7000 - val_class_out_loss: 1.1765 - val_freshness_scaled_loss: 3.0979 - val_freshness_scaled_mae: 1.5269 - val_freshness_scaled_mse: 3.0325 - val_loss: 1.4740 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - class_out_accuracy: 0.4328 - class_out_categorical_accuracy: 0.4328 - class_out_loss: 1.3926 - freshness_scaled_loss: 6.6746 - freshness_scaled_mae: 1.9953 - freshness_scaled_mse: 6.6847 - loss: 2.0613 - val_class_out_accuracy: 0.7028 - val_class_out_categorical_accuracy: 0.7028 - val_class_out_loss: 1.1517 - val_freshness_scaled_loss: 2.9269 - val_freshness_scaled_mae: 1.4783 - val_freshness_scaled_mse: 2.8641 - val_loss: 1.4338 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - class_out_accuracy: 0.4506 - class_out_categorical_accuracy: 0.4506 - class_out_loss: 1.3742 - freshness_scaled_loss: 6.0428 - freshness_scaled_mae: 1.8950 - freshness_scaled_mse: 6.0513 - loss: 1.9802 - val_class_out_accuracy: 0.7167 - val_class_out_categorical_accuracy: 0.7167 - val_class_out_loss: 1.1155 - val_freshness_scaled_loss: 2.7048 - val_freshness_scaled_mae: 1.4070 - val_freshness_scaled_mse: 2.6505 - val_loss: 1.3757 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.4864 - class_out_categorical_accuracy: 0.4864 - class_out_loss: 1.3095 - freshness_scaled_loss: 6.0539 - freshness_scaled_mae: 1.8730 - freshness_scaled_mse: 6.0562 - loss: 1.9164 - val_class_out_accuracy: 0.7250 - val_class_out_categorical_accuracy: 0.7250 - val_class_out_loss: 1.0771 - val_freshness_scaled_loss: 2.6639 - val_freshness_scaled_mae: 1.3862 - val_freshness_scaled_mse: 2.6070 - val_loss: 1.3332 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.5079 - class_out_categorical_accuracy: 0.5079 - class_out_loss: 1.3319 - freshness_scaled_loss: 6.2131 - freshness_scaled_mae: 1.9192 - freshness_scaled_mse: 6.2175 - loss: 1.9549 - val_class_out_accuracy: 0.7333 - val_class_out_categorical_accuracy: 0.7333 - val_class_out_loss: 1.0342 - val_freshness_scaled_loss: 2.6205 - val_freshness_scaled_mae: 1.3633 - val_freshness_scaled_mse: 2.5645 - val_loss: 1.2877 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - class_out_accuracy: 0.5494 - class_out_categorical_accuracy: 0.5494 - class_out_loss: 1.2177 - freshness_scaled_loss: 5.2199 - freshness_scaled_mae: 1.7545 - freshness_scaled_mse: 5.2279 - loss: 1.7387 - val_class_out_accuracy: 0.7278 - val_class_out_categorical_accuracy: 0.7278 - val_class_out_loss: 0.9923 - val_freshness_scaled_loss: 2.5684 - val_freshness_scaled_mae: 1.3310 - val_freshness_scaled_mse: 2.5140 - val_loss: 1.2399 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.5665 - class_out_categorical_accuracy: 0.5665 - class_out_loss: 1.1718 - freshness_scaled_loss: 5.5499 - freshness_scaled_mae: 1.7843 - freshness_scaled_mse: 5.5240 - loss: 1.7245 - val_class_out_accuracy: 0.7333 - val_class_out_categorical_accuracy: 0.7333 - val_class_out_loss: 0.9526 - val_freshness_scaled_loss: 2.5376 - val_freshness_scaled_mae: 1.3086 - val_freshness_scaled_mse: 2.4866 - val_loss: 1.1976 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.6052 - class_out_categorical_accuracy: 0.6052 - class_out_loss: 1.1686 - freshness_scaled_loss: 5.3418 - freshness_scaled_mae: 1.7541 - freshness_scaled_mse: 5.3552 - loss: 1.7036 - val_class_out_accuracy: 0.7278 - val_class_out_categorical_accuracy: 0.7278 - val_class_out_loss: 0.9020 - val_freshness_scaled_loss: 2.5254 - val_freshness_scaled_mae: 1.3013 - val_freshness_scaled_mse: 2.4738 - val_loss: 1.1437 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - class_out_accuracy: 0.6302 - class_out_categorical_accuracy: 0.6302 - class_out_loss: 1.0839 - freshness_scaled_loss: 5.2663 - freshness_scaled_mae: 1.7521 - freshness_scaled_mse: 5.2441 - loss: 1.6081 - val_class_out_accuracy: 0.7333 - val_class_out_categorical_accuracy: 0.7333 - val_class_out_loss: 0.8582 - val_freshness_scaled_loss: 2.4326 - val_freshness_scaled_mae: 1.2628 - val_freshness_scaled_mse: 2.3997 - val_loss: 1.0942 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.6502 - class_out_categorical_accuracy: 0.6502 - class_out_loss: 1.0435 - freshness_scaled_loss: 4.5962 - freshness_scaled_mae: 1.6176 - freshness_scaled_mse: 4.6064 - loss: 1.5050 - val_class_out_accuracy: 0.7361 - val_class_out_categorical_accuracy: 0.7361 - val_class_out_loss: 0.8397 - val_freshness_scaled_loss: 2.4196 - val_freshness_scaled_mae: 1.2513 - val_freshness_scaled_mse: 2.3940 - val_loss: 1.0770 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - class_out_accuracy: 0.6609 - class_out_categorical_accuracy: 0.6609 - class_out_loss: 1.0120 - freshness_scaled_loss: 4.8490 - freshness_scaled_mae: 1.6790 - freshness_scaled_mse: 4.8433 - loss: 1.4970 - val_class_out_accuracy: 0.7333 - val_class_out_categorical_accuracy: 0.7333 - val_class_out_loss: 0.8243 - val_freshness_scaled_loss: 2.4068 - val_freshness_scaled_mae: 1.2386 - val_freshness_scaled_mse: 2.4063 - val_loss: 1.0640 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - class_out_accuracy: 0.6609 - class_out_categorical_accuracy: 0.6609 - class_out_loss: 0.9480 - freshness_scaled_loss: 4.4356 - freshness_scaled_mae: 1.6053 - freshness_scaled_mse: 4.4356 - loss: 1.3927 - val_class_out_accuracy: 0.7444 - val_class_out_categorical_accuracy: 0.7444 - val_class_out_loss: 0.7706 - val_freshness_scaled_loss: 2.3916 - val_freshness_scaled_mae: 1.2561 - val_freshness_scaled_mse: 2.3733 - val_loss: 1.0066 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - class_out_accuracy: 0.7124 - class_out_categorical_accuracy: 0.7124 - class_out_loss: 0.8547 - freshness_scaled_loss: 4.5291 - freshness_scaled_mae: 1.6085 - freshness_scaled_mse: 4.4938 - loss: 1.3040 - val_class_out_accuracy: 0.7417 - val_class_out_categorical_accuracy: 0.7417 - val_class_out_loss: 0.7168 - val_freshness_scaled_loss: 2.3835 - val_freshness_scaled_mae: 1.2420 - val_freshness_scaled_mse: 2.3828 - val_loss: 0.9520 - learning_rate: 1.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - class_out_accuracy: 0.7074 - class_out_categorical_accuracy: 0.7074 - class_out_loss: 0.8749 - freshness_scaled_loss: 4.5628 - freshness_scaled_mae: 1.6247 - freshness_scaled_mse: 4.5501 - loss: 1.3302 - val_class_out_accuracy: 0.7583 - val_class_out_categorical_accuracy: 0.7583 - val_class_out_loss: 0.7067 - val_freshness_scaled_loss: 2.3603 - val_freshness_scaled_mae: 1.2417 - val_freshness_scaled_mse: 2.3497 - val_loss: 0.9389 - learning_rate: 1.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.7217 - class_out_categorical_accuracy: 0.7217 - class_out_loss: 0.8319 - freshness_scaled_loss: 4.3820 - freshness_scaled_mae: 1.5686 - freshness_scaled_mse: 4.3944 - loss: 1.2715 - val_class_out_accuracy: 0.7722 - val_class_out_categorical_accuracy: 0.7722 - val_class_out_loss: 0.6454 - val_freshness_scaled_loss: 2.3339 - val_freshness_scaled_mae: 1.2343 - val_freshness_scaled_mse: 2.3415 - val_loss: 0.8746 - learning_rate: 1.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - class_out_accuracy: 0.7403 - class_out_categorical_accuracy: 0.7403 - class_out_loss: 0.7840 - freshness_scaled_loss: 4.2361 - freshness_scaled_mae: 1.5546 - freshness_scaled_mse: 4.2326 - loss: 1.2069 - val_class_out_accuracy: 0.7528 - val_class_out_categorical_accuracy: 0.7528 - val_class_out_loss: 0.6175 - val_freshness_scaled_loss: 2.3198 - val_freshness_scaled_mae: 1.2057 - val_freshness_scaled_mse: 2.3402 - val_loss: 0.8457 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Saving model...\n",
      "âœ… Model saved as 'improved_produce_quality_model.h5'\n",
      "\n",
      "Step 8: Evaluating model...\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - class_out_accuracy: 0.7722 - class_out_categorical_accuracy: 0.7722 - class_out_loss: 0.6454 - freshness_scaled_loss: 2.3339 - freshness_scaled_mae: 1.2343 - freshness_scaled_mse: 2.3415 - loss: 0.8746\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Class Accuracy: 0.7722\n",
      "Class Loss: 0.6454\n",
      "Freshness MAE: 1.2343\n",
      "\n",
      "ðŸŽ¯ Starting Streamlit Dashboard...\n",
      "ðŸ‘‰ Run this command in your terminal to start the dashboard:\n",
      "   streamlit run your_script_name.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 13:22:11.456 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.458 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.766 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\bbuser\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-05 13:22:11.769 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.781 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.783 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.785 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.786 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.787 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.787 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.788 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-05 13:22:11.791 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.799 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.812 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.815 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.816 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.817 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-05 13:22:11.818 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Database initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# ===== COMPLETE FRUITS & VEGETABLES QUALITY ASSESSMENT SYSTEM =====\n",
    "\n",
    "# ===== SECTION 1: IMPORTS AND CONFIGURATION =====\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import sqlite3\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "# **YOUR DATASET PATHS**\n",
    "BASE_DIR = r\"C:\\Users\\bbuser\\Desktop\\FRUITS & VEGETABLES\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "# **CLASS MAPPINGS FOR YOUR FOLDER STRUCTURE**\n",
    "quality_class_mapping = {\n",
    "    # Fruits - Unripe (0)\n",
    "    'apple_unripe': 0, 'banana_unripe': 0, 'mango_unripe': 0,\n",
    "    # Fruits - Ripe (1)\n",
    "    'apple_ripe': 1, 'banana_ripe': 1, 'mango_ripe': 1,\n",
    "    # Fruits - Overripe (2)\n",
    "    'apple_overripe': 2, 'banana_overripe': 2, 'mango_overripe': 2,\n",
    "    # Fruits - Bruised (3)\n",
    "    'apple_bruised': 3, 'banana_bruised': 3, 'mango_bruised': 3,\n",
    "    # Vegetables - Unripe (0)\n",
    "    'cucumber_unripe': 0, 'tomato_unripe': 0,\n",
    "    # Vegetables - Ripe (1)\n",
    "    'cucumber_ripe': 1, 'tomato_ripe': 1,\n",
    "    # Vegetables - Overripe (2)\n",
    "    'cucumber_overripe': 2, 'tomato_overripe': 2,\n",
    "    # Vegetables - Bruised (3)\n",
    "    'cucumber_bruised': 3, 'tomato_bruised': 3\n",
    "}\n",
    "\n",
    "quality_class_names = {\n",
    "    0: 'unripe',\n",
    "    1: 'ripe', \n",
    "    2: 'overripe',\n",
    "    3: 'bruised'\n",
    "}\n",
    "\n",
    "class_mapping = quality_class_mapping\n",
    "\n",
    "# ===== SECTION 2: SQLITE DATABASE SETUP =====\n",
    "\n",
    "def init_database():\n",
    "    \"\"\"Initialize SQLite database for storing predictions\"\"\"\n",
    "    conn = sqlite3.connect('produce_quality.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS predictions (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT NOT NULL,\n",
    "            image_path TEXT,\n",
    "            predicted_class INTEGER NOT NULL,\n",
    "            predicted_class_name TEXT NOT NULL,\n",
    "            freshness_score REAL NOT NULL,\n",
    "            confidence REAL NOT NULL,\n",
    "            color_features TEXT,\n",
    "            processed_image_path TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS model_performance (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT NOT NULL,\n",
    "            accuracy REAL NOT NULL,\n",
    "            freshness_mae REAL NOT NULL,\n",
    "            training_loss REAL NOT NULL,\n",
    "            validation_loss REAL NOT NULL\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"âœ… Database initialized successfully\")\n",
    "\n",
    "def save_prediction_to_db(image_path, predicted_class, freshness_score, confidence, color_features, processed_image_path=None):\n",
    "    \"\"\"Save prediction results to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect('produce_quality.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    timestamp = datetime.now().isoformat()\n",
    "    predicted_class_name = quality_class_names.get(predicted_class, 'unknown')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        INSERT INTO predictions \n",
    "        (timestamp, image_path, predicted_class, predicted_class_name, freshness_score, confidence, color_features, processed_image_path)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (timestamp, image_path, predicted_class, predicted_class_name, freshness_score, confidence, str(color_features), processed_image_path))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return cursor.lastrowid\n",
    "\n",
    "def get_recent_predictions(limit=10):\n",
    "    \"\"\"Get recent predictions from database\"\"\"\n",
    "    conn = sqlite3.connect('produce_quality.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT * FROM predictions \n",
    "        ORDER BY timestamp DESC \n",
    "        LIMIT ?\n",
    "    ''', (limit,))\n",
    "    \n",
    "    predictions = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return predictions\n",
    "\n",
    "def get_prediction_stats():\n",
    "    \"\"\"Get prediction statistics from database\"\"\"\n",
    "    conn = sqlite3.connect('produce_quality.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Total predictions\n",
    "    cursor.execute('SELECT COUNT(*) FROM predictions')\n",
    "    total_predictions = cursor.fetchone()[0]\n",
    "    \n",
    "    # Class distribution\n",
    "    cursor.execute('''\n",
    "        SELECT predicted_class_name, COUNT(*) \n",
    "        FROM predictions \n",
    "        GROUP BY predicted_class_name\n",
    "    ''')\n",
    "    class_distribution = cursor.fetchall()\n",
    "    \n",
    "    # Average freshness\n",
    "    cursor.execute('SELECT AVG(freshness_score) FROM predictions')\n",
    "    avg_freshness = cursor.fetchone()[0]\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'total_predictions': total_predictions,\n",
    "        'class_distribution': class_distribution,\n",
    "        'avg_freshness': avg_freshness\n",
    "    }\n",
    "\n",
    "# ===== SECTION 3: DATASET PREPARATION FUNCTIONS =====\n",
    "\n",
    "def create_train_val_splits(test_size=0.2):\n",
    "    \"\"\"Create train/validation splits from your folder structure\"\"\"\n",
    "    base_path = Path(BASE_DIR)\n",
    "    train_path = Path(TRAIN_DIR)\n",
    "    val_path = Path(VAL_DIR)\n",
    "    \n",
    "    # Create train/val directories\n",
    "    train_path.mkdir(exist_ok=True)\n",
    "    val_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ“ Creating train/validation splits...\")\n",
    "    \n",
    "    # Process each fruit and vegetable\n",
    "    for category in ['fruits', 'vegetables']:\n",
    "        category_path = base_path / category\n",
    "        \n",
    "        if not category_path.exists():\n",
    "            print(f\"âš ï¸ Warning: {category_path} not found\")\n",
    "            continue\n",
    "            \n",
    "        for item in category_path.iterdir():\n",
    "            if item.is_dir():  # apple, banana, mango, cucumber, tomato\n",
    "                for quality in ['ripe', 'unripe', 'overripe', 'bruised']:\n",
    "                    quality_path = item / quality\n",
    "                    \n",
    "                    if quality_path.exists():\n",
    "                        # Get all images\n",
    "                        images = list(quality_path.glob(\"*.*\"))\n",
    "                        images = [img for img in images if img.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                        \n",
    "                        if images:\n",
    "                            # Split into train/val\n",
    "                            train_imgs, val_imgs = train_test_split(images, test_size=test_size, random_state=42)\n",
    "                            \n",
    "                            # Create target directories\n",
    "                            train_target = train_path / f\"{item.name}_{quality}\"\n",
    "                            val_target = val_path / f\"{item.name}_{quality}\"\n",
    "                            train_target.mkdir(parents=True, exist_ok=True)\n",
    "                            val_target.mkdir(parents=True, exist_ok=True)\n",
    "                            \n",
    "                            # Copy images\n",
    "                            for img in train_imgs:\n",
    "                                shutil.copy2(img, train_target / img.name)\n",
    "                            for img in val_imgs:\n",
    "                                shutil.copy2(img, val_target / img.name)\n",
    "                            \n",
    "                            print(f\"  âœ… {item.name}_{quality}: {len(train_imgs)} train, {len(val_imgs)} val\")\n",
    "                        else:\n",
    "                            print(f\"  âš ï¸ No images in: {item.name}_{quality}\")\n",
    "                    else:\n",
    "                        print(f\"  âš ï¸ Missing: {item.name}_{quality}\")\n",
    "    \n",
    "    print(\"ðŸŽ‰ Train/validation splits created successfully!\")\n",
    "\n",
    "def check_dataset_structure():\n",
    "    \"\"\"Check if your dataset structure is correct\"\"\"\n",
    "    base_path = Path(BASE_DIR)\n",
    "    \n",
    "    print(\"ðŸ” Checking dataset structure...\")\n",
    "    \n",
    "    expected_folders = ['fruits', 'vegetables']\n",
    "    fruits_items = ['apple', 'banana', 'mango']\n",
    "    vegetables_items = ['cucumber', 'tomato']\n",
    "    quality_levels = ['ripe', 'unripe', 'overripe', 'bruised']\n",
    "    \n",
    "    total_images = 0\n",
    "    \n",
    "    for folder in expected_folders:\n",
    "        folder_path = base_path / folder\n",
    "        if folder_path.exists():\n",
    "            print(f\"âœ… Found: {folder}\")\n",
    "            \n",
    "            if folder == 'fruits':\n",
    "                items = fruits_items\n",
    "            else:\n",
    "                items = vegetables_items\n",
    "                \n",
    "            for item in items:\n",
    "                item_path = folder_path / item\n",
    "                if item_path.exists():\n",
    "                    print(f\"  âœ… Found: {item}\")\n",
    "                    for quality in quality_levels:\n",
    "                        quality_path = item_path / quality\n",
    "                        if quality_path.exists():\n",
    "                            images = list(quality_path.glob(\"*.*\"))\n",
    "                            images = [img for img in images if img.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                            total_images += len(images)\n",
    "                            print(f\"    âœ… {quality}: {len(images)} images\")\n",
    "                        else:\n",
    "                            print(f\"    âŒ Missing: {quality}\")\n",
    "                else:\n",
    "                    print(f\"  âŒ Missing: {item}\")\n",
    "        else:\n",
    "            print(f\"âŒ Missing: {folder}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š TOTAL IMAGES FOUND: {total_images}\")\n",
    "    return total_images > 0\n",
    "\n",
    "# ===== SECTION 4: IMAGE PROCESSING FUNCTIONS =====\n",
    "\n",
    "def enhanced_bg_mask_rgb(arr):\n",
    "    \"\"\"Improved background removal using color thresholding\"\"\"\n",
    "    try:\n",
    "        # Convert to HSV for better color segmentation\n",
    "        hsv = cv2.cvtColor(arr, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define color ranges for common background colors\n",
    "        lower_white1 = np.array([0, 0, 200])\n",
    "        upper_white1 = np.array([180, 55, 255])\n",
    "        \n",
    "        lower_white2 = np.array([0, 0, 150])\n",
    "        upper_white2 = np.array([180, 80, 255])\n",
    "        \n",
    "        # Create masks for different background types\n",
    "        mask1 = cv2.inRange(hsv, lower_white1, upper_white1)\n",
    "        mask2 = cv2.inRange(hsv, lower_white2, upper_white2)\n",
    "        \n",
    "        # Combine masks\n",
    "        background_mask = cv2.bitwise_or(mask1, mask2)\n",
    "        foreground_mask = cv2.bitwise_not(background_mask)\n",
    "        \n",
    "        # Clean up mask using morphology\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Apply mask to get only the produce\n",
    "        masked = cv2.bitwise_and(arr, arr, mask=foreground_mask)\n",
    "        \n",
    "        return masked, foreground_mask\n",
    "    except Exception as e:\n",
    "        print(f\"Error in background removal: {e}\")\n",
    "        return arr, np.ones(arr.shape[:2], dtype=np.uint8) * 255\n",
    "\n",
    "def extract_color_features_lab(arr, mask):\n",
    "    \"\"\"Extract color statistics in LAB space\"\"\"\n",
    "    try:\n",
    "        lab = cv2.cvtColor(arr, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Apply mask to get only produce pixels\n",
    "        masked_lab = lab[mask > 0]\n",
    "        \n",
    "        if len(masked_lab) == 0:\n",
    "            return np.zeros(12)\n",
    "        \n",
    "        # Extract color statistics\n",
    "        l_channel = masked_lab[:, 0]  # Lightness\n",
    "        a_channel = masked_lab[:, 1]  # Green-Red \n",
    "        b_channel = masked_lab[:, 2]  # Blue-Yellow\n",
    "        \n",
    "        # Basic statistics\n",
    "        l_mean, a_mean, b_mean = np.mean(masked_lab, axis=0)\n",
    "        l_std, a_std, b_std = np.std(masked_lab, axis=0)\n",
    "        \n",
    "        # Additional features\n",
    "        l_median = np.median(l_channel)\n",
    "        color_variance = np.var(masked_lab, axis=0)\n",
    "        \n",
    "        # Simple histogram features\n",
    "        hist_l = np.histogram(l_channel, bins=8, range=(0, 255))[0]\n",
    "        hist_l = hist_l / (np.sum(hist_l) + 1e-6)\n",
    "        \n",
    "        # Total: 12 features\n",
    "        features = np.array([\n",
    "            l_mean, a_mean, b_mean,      # Color means\n",
    "            l_std, a_std, b_std,         # Color standard deviations\n",
    "            l_median,                    # Lightness median\n",
    "            color_variance[0],           # Lightness variance\n",
    "            np.mean(color_variance[1:]), # Average color variance\n",
    "        ] + hist_l[:3].tolist())         # First 3 histogram bins\n",
    "        \n",
    "        # Ensure exactly 12 features\n",
    "        if len(features) != 12:\n",
    "            if len(features) > 12:\n",
    "                features = features[:12]\n",
    "            else:\n",
    "                features = np.pad(features, (0, 12 - len(features)), mode='constant')\n",
    "        \n",
    "        return features.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature extraction: {e}\")\n",
    "        return np.zeros(12, dtype=np.float32)\n",
    "\n",
    "def calculate_real_freshness(img_array, mask, quality_class):\n",
    "    \"\"\"Calculate freshness based on image analysis\"\"\"\n",
    "    try:\n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor(img_array, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Apply mask to get only produce pixels\n",
    "        masked_lab = lab[mask > 0]\n",
    "        \n",
    "        if len(masked_lab) == 0:\n",
    "            return 5.0  # Default if no foreground detected\n",
    "        \n",
    "        # Extract color statistics\n",
    "        l_channel = masked_lab[:, 0]\n",
    "        a_channel = masked_lab[:, 1]\n",
    "        b_channel = masked_lab[:, 2]\n",
    "        \n",
    "        l_mean = np.mean(l_channel)\n",
    "        l_std = np.std(l_channel)\n",
    "        a_mean = np.mean(a_channel)\n",
    "        b_mean = np.mean(b_channel)\n",
    "        \n",
    "        # Calculate freshness indicators\n",
    "        color_saturation = np.sqrt(a_mean**2 + b_mean**2)\n",
    "        color_uniformity = 1.0 / (l_std + 1e-6)\n",
    "        \n",
    "        # Base freshness calculation\n",
    "        base_freshness = 5.0\n",
    "        \n",
    "        # Lightness adjustment\n",
    "        if 100 <= l_mean <= 160:\n",
    "            base_freshness += 1.5\n",
    "        elif l_mean < 60:\n",
    "            base_freshness -= 1.5\n",
    "        elif l_mean > 200:\n",
    "            base_freshness -= 0.5\n",
    "        \n",
    "        # Color saturation adjustment\n",
    "        if color_saturation > 30:\n",
    "            base_freshness += 1.0\n",
    "        elif color_saturation < 15:\n",
    "            base_freshness -= 1.0\n",
    "        \n",
    "        # Color uniformity adjustment\n",
    "        if color_uniformity > 0.1:\n",
    "            base_freshness += min(1.5, color_uniformity * 2)\n",
    "        \n",
    "        # Quality class-specific adjustments\n",
    "        quality_adjustments = {\n",
    "            'unripe': -0.5,\n",
    "            'ripe': +1.5,\n",
    "            'overripe': -2.0,\n",
    "            'bruised': -2.5,\n",
    "            'unknown': 0.0\n",
    "        }\n",
    "        \n",
    "        base_freshness += quality_adjustments.get(quality_class.lower(), 0.0)\n",
    "        \n",
    "        # Constrain to 0-10 range\n",
    "        freshness_score = max(0.0, min(10.0, base_freshness))\n",
    "        \n",
    "        return freshness_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in freshness calculation: {e}\")\n",
    "        return 5.0  # Fallback score\n",
    "\n",
    "def load_and_preprocess_with_freshness(path, class_mapping):\n",
    "    \"\"\"Complete preprocessing with image analysis\"\"\"\n",
    "    try:\n",
    "        # Convert Tensor to string path\n",
    "        if isinstance(path, tf.Tensor):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "\n",
    "        # Load and process image\n",
    "        img = Image.open(path).convert('RGB').resize(IMG_SIZE)\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        masked, mask = enhanced_bg_mask_rgb(arr)\n",
    "        stats = extract_color_features_lab(arr, mask)\n",
    "        \n",
    "        # Use MobileNetV2 preprocessing\n",
    "        img_f = tf.keras.applications.mobilenet_v2.preprocess_input(arr)\n",
    "        \n",
    "        # Extract class from path\n",
    "        class_folder = Path(path).parent.name\n",
    "        label_idx = class_mapping.get(class_folder.lower(), 0)  # Convert to lowercase for matching\n",
    "        \n",
    "        # Determine quality class\n",
    "        quality_class = 'unknown'\n",
    "        class_lower = class_folder.lower()\n",
    "        \n",
    "        if 'unripe' in class_lower:\n",
    "            quality_class = 'unripe'\n",
    "        elif 'ripe' in class_lower:\n",
    "            quality_class = 'ripe'\n",
    "        elif 'overripe' in class_lower:\n",
    "            quality_class = 'overripe'\n",
    "        elif 'bruised' in class_lower:\n",
    "            quality_class = 'bruised'\n",
    "        \n",
    "        # Calculate freshness score\n",
    "        freshness_score = calculate_real_freshness(arr, mask, quality_class)\n",
    "        \n",
    "        return img_f, stats, label_idx, freshness_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        # Return default values on error\n",
    "        img_f = np.zeros((*IMG_SIZE, 3), dtype=np.float32)\n",
    "        img_f = tf.keras.applications.mobilenet_v2.preprocess_input(img_f)\n",
    "        stats = np.zeros(12, dtype=np.float32)\n",
    "        label_idx = 0\n",
    "        freshness_score = 5.0\n",
    "        return img_f, stats, label_idx, freshness_score\n",
    "\n",
    "# ===== SECTION 5: DATASET MANAGEMENT =====\n",
    "\n",
    "def analyze_dataset_balance(root_dir):\n",
    "    \"\"\"Analyze dataset balance for the 4 quality classes\"\"\"\n",
    "    class_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    detailed_counts = {}\n",
    "    \n",
    "    for class_folder in Path(root_dir).iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name.lower()\n",
    "            quality_class = class_mapping.get(class_name)\n",
    "            \n",
    "            if quality_class is not None:\n",
    "                images = list(class_folder.glob(\"*.*\"))\n",
    "                images = [img for img in images if img.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                class_counts[quality_class] += len(images)\n",
    "                detailed_counts[class_name] = len(images)\n",
    "    \n",
    "    print(\"=== DATASET BALANCE ANALYSIS ===\")\n",
    "    total_images = sum(class_counts.values())\n",
    "    \n",
    "    for quality_class, count in class_counts.items():\n",
    "        class_name = quality_class_names[quality_class]\n",
    "        percentage = (count / total_images) * 100 if total_images > 0 else 0\n",
    "        print(f\"{class_name}: {count} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"Total images: {total_images}\")\n",
    "    \n",
    "    # Identify imbalanced classes\n",
    "    print(\"\\n=== IMBALANCE ANALYSIS ===\")\n",
    "    for class_name, count in detailed_counts.items():\n",
    "        if count < 20:  # Critical threshold\n",
    "            print(f\"ðŸš¨ CRITICALLY IMBALANCED: {class_name}: {count} images\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "def calculate_class_weights(train_dir):\n",
    "    \"\"\"Calculate class weights for imbalanced dataset\"\"\"\n",
    "    class_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    \n",
    "    for class_folder in Path(train_dir).iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name.lower()\n",
    "            quality_class = class_mapping.get(class_name)\n",
    "            \n",
    "            if quality_class is not None:\n",
    "                images = list(class_folder.glob(\"*.*\"))\n",
    "                images = [img for img in images if img.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                class_counts[quality_class] += len(images)\n",
    "    \n",
    "    total = sum(class_counts.values())\n",
    "    class_weights = {}\n",
    "    \n",
    "    print(\"=== CLASS WEIGHTS ===\")\n",
    "    for class_idx, count in class_counts.items():\n",
    "        weight = total / (len(class_counts) * count) if count > 0 else 1.0\n",
    "        class_weights[class_idx] = weight\n",
    "        print(f\"Class {quality_class_names[class_idx]}: {count} samples -> weight: {weight:.2f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def create_dataset_from_structure(root_dir, batch_size=32, shuffle=True):\n",
    "    \"\"\"Create dataset from your specific folder structure\"\"\"\n",
    "    filepaths = []\n",
    "    quality_labels = []\n",
    "    \n",
    "    root_path = Path(root_dir)\n",
    "    \n",
    "    print(f\"ðŸ“ Loading dataset from: {root_dir}\")\n",
    "    \n",
    "    # Process all class folders directly (after train/val split)\n",
    "    for class_folder in root_path.iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name.lower()\n",
    "            quality_class = class_mapping.get(class_name)\n",
    "            \n",
    "            if quality_class is not None:\n",
    "                images = list(class_folder.glob(\"*.*\"))\n",
    "                images = [img for img in images if img.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                \n",
    "                for img_path in images:\n",
    "                    filepaths.append(str(img_path))\n",
    "                    quality_labels.append(quality_class)\n",
    "                \n",
    "                print(f\"  âœ… {class_name}: {len(images)} images -> class {quality_class}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ Unknown class: {class_name}\")\n",
    "    \n",
    "    if not filepaths:\n",
    "        raise ValueError(f\"No images found in {root_dir}\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Total images: {len(filepaths)}\")\n",
    "    print(f\"ðŸ“Š Class distribution: {np.bincount(quality_labels)}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((filepaths, quality_labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(filepaths))\n",
    "    \n",
    "    def load_and_map(path, quality_label):\n",
    "        # Use improved preprocessing\n",
    "        img_f, stats, _, freshness = tf.py_function(\n",
    "            func=lambda p: load_and_preprocess_with_freshness(p, class_mapping),\n",
    "            inp=[path],\n",
    "            Tout=[tf.float32, tf.float32, tf.int32, tf.float32]\n",
    "        )\n",
    "        \n",
    "        # Set proper shapes\n",
    "        img_f.set_shape([224, 224, 3])\n",
    "        stats.set_shape([12])\n",
    "        \n",
    "        return (img_f, stats), (tf.one_hot(quality_label, depth=4), tf.reshape(freshness, [1]))\n",
    "    \n",
    "    ds = ds.map(load_and_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# ===== SECTION 6: MODEL ARCHITECTURE =====\n",
    "\n",
    "def create_improved_multi_output_model(num_classes=4):\n",
    "    \"\"\"Create improved model with proper architecture\"\"\"\n",
    "    \n",
    "    print(f\"Building improved model with {num_classes} classes\")\n",
    "    \n",
    "    # Image input with MobileNetV2 preprocessing\n",
    "    img_input = tf.keras.Input(shape=(224, 224, 3), name='img_input')\n",
    "    \n",
    "    # Base model with proper preprocessing and unfrozen layers\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=img_input\n",
    "    )\n",
    "    \n",
    "    # Unfreeze the top layers for fine-tuning\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:  # Freeze bottom layers\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(f\"Base model layers: {len(base_model.layers)}\")\n",
    "    print(f\"Trainable layers: {sum([l.trainable for l in base_model.layers])}\")\n",
    "    \n",
    "    # Image features\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Stats features (simplified)\n",
    "    stats_input = tf.keras.Input(shape=(12,), name='stats_input')\n",
    "    stats_branch = tf.keras.layers.Dense(32, activation='relu')(stats_input)\n",
    "    stats_branch = tf.keras.layers.Dropout(0.2)(stats_branch)\n",
    "    \n",
    "    # Combine features\n",
    "    combined = tf.keras.layers.concatenate([x, stats_branch])\n",
    "    \n",
    "    # Shared layers\n",
    "    combined = tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "    combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "    combined = tf.keras.layers.Dropout(0.3)(combined)\n",
    "    \n",
    "    # Outputs - 4 classes for quality\n",
    "    class_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='class_out')(combined)\n",
    "    \n",
    "    # Freshness output (0-10 scale)\n",
    "    freshness_output = tf.keras.layers.Dense(1, activation='sigmoid', name='freshness_out')(combined)\n",
    "    freshness_output = tf.keras.layers.Lambda(lambda x: x * 10.0, name='freshness_scaled')(freshness_output)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[img_input, stats_input],\n",
    "        outputs=[class_output, freshness_output]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_training_callbacks():\n",
    "    \"\"\"Get training callbacks for better convergence\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_class_out_accuracy',\n",
    "            mode='max',  # Higher accuracy is better\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_class_out_loss',\n",
    "            mode='min',  # Lower loss is better\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# ===== SECTION 7: TRAINING AND EVALUATION =====\n",
    "\n",
    "def train_and_evaluate_model():\n",
    "    \"\"\"Main training and evaluation function\"\"\"\n",
    "    \n",
    "    # Step 1: Create train/val splits if they don't exist\n",
    "    print(\"Step 1: Checking train/validation splits...\")\n",
    "    if not os.path.exists(TRAIN_DIR) or not os.path.exists(VAL_DIR):\n",
    "        print(\"Creating train/val splits...\")\n",
    "        create_train_val_splits(test_size=0.2)\n",
    "    else:\n",
    "        print(\"âœ… Train/val splits already exist\")\n",
    "    \n",
    "    # Step 2: Analyze dataset balance\n",
    "    print(\"\\nStep 2: Analyzing dataset balance...\")\n",
    "    train_class_counts = analyze_dataset_balance(TRAIN_DIR)\n",
    "    val_class_counts = analyze_dataset_balance(VAL_DIR)\n",
    "    \n",
    "    # Step 3: Calculate class weights\n",
    "    print(\"\\nStep 3: Calculating class weights...\")\n",
    "    class_weights = calculate_class_weights(TRAIN_DIR)\n",
    "    \n",
    "    # Step 4: Create datasets\n",
    "    print(\"\\nStep 4: Creating dataset pipelines...\")\n",
    "    try:\n",
    "        train_ds = create_dataset_from_structure(TRAIN_DIR, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_ds = create_dataset_from_structure(VAL_DIR, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        print(\"âœ… Datasets created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating datasets: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 5: Create and compile model\n",
    "    print(\"\\nStep 5: Building and compiling model...\")\n",
    "    model = create_improved_multi_output_model(num_classes=4)\n",
    "    \n",
    "    # Custom loss weights to focus on classification\n",
    "    loss_weights = {\n",
    "        'class_out': 1.0,      # Primary focus\n",
    "        'freshness_scaled': 0.1  # Secondary task\n",
    "    }\n",
    "    \n",
    "    # Compile with different learning rates for different outputs\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            'class_out': 'categorical_crossentropy',\n",
    "            'freshness_scaled': 'mse'\n",
    "        },\n",
    "        loss_weights=loss_weights,\n",
    "        metrics={\n",
    "            'class_out': ['accuracy', 'categorical_accuracy'],\n",
    "            'freshness_scaled': ['mae', 'mse']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Model compiled successfully!\")\n",
    "    \n",
    "    # Step 6: Train model\n",
    "    print(\"\\nStep 6: Training model...\")\n",
    "    print(f\"Training for {EPOCHS} epochs with early stopping\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=get_training_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Step 7: Save model\n",
    "    print(\"\\nStep 7: Saving model...\")\n",
    "    model.save('improved_produce_quality_model.h5')\n",
    "    print(\"âœ… Model saved as 'improved_produce_quality_model.h5'\")\n",
    "    \n",
    "    # Step 8: Evaluate model\n",
    "    print(\"\\nStep 8: Evaluating model...\")\n",
    "    test_results = model.evaluate(val_ds, verbose=1)\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Class Accuracy: {test_results[3]:.4f}\")\n",
    "    print(f\"Class Loss: {test_results[1]:.4f}\")\n",
    "    print(f\"Freshness MAE: {test_results[5]:.4f}\")\n",
    "    \n",
    "    return model, history, test_results\n",
    "\n",
    "def make_predictions(model, num_samples=5):\n",
    "    \"\"\"Make sample predictions on validation data\"\"\"\n",
    "    val_ds = create_dataset_from_structure(VAL_DIR, batch_size=32, shuffle=False)\n",
    "    \n",
    "    sample_batch = next(iter(val_ds))\n",
    "    (sample_images, sample_stats), (true_classes, true_freshness) = sample_batch\n",
    "    \n",
    "    class_pred, freshness_pred = model.predict([sample_images, sample_stats], verbose=0)\n",
    "    \n",
    "    # Convert predictions\n",
    "    pred_classes = np.argmax(class_pred, axis=1)\n",
    "    true_class_indices = np.argmax(true_classes, axis=1)\n",
    "    \n",
    "    print(f\"\\nSample predictions (first {num_samples}):\")\n",
    "    print(f\"True classes: {true_class_indices[:num_samples]}\")\n",
    "    print(f\"Pred classes: {pred_classes[:num_samples]}\")\n",
    "    print(f\"True freshness: {true_freshness.numpy()[:num_samples].flatten()}\")\n",
    "    print(f\"Pred freshness: {freshness_pred[:num_samples].flatten()}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    class_accuracy = np.mean(pred_classes == true_class_indices)\n",
    "    freshness_mae = np.mean(np.abs(freshness_pred.flatten() - true_freshness.numpy().flatten()))\n",
    "    \n",
    "    print(f\"\\nSample Batch Class Accuracy: {class_accuracy:.4f}\")\n",
    "    print(f\"Sample Batch Freshness MAE: {freshness_mae:.4f}\")\n",
    "    \n",
    "    return class_accuracy, freshness_mae\n",
    "\n",
    "# ===== SECTION 8: PREDICTION AND STREAMLIT FUNCTIONS =====\n",
    "\n",
    "def predict_single_image(model, image_path):\n",
    "    \"\"\"Predict quality and freshness for a single image\"\"\"\n",
    "    try:\n",
    "        # Preprocess image\n",
    "        img_f, stats, _, _ = load_and_preprocess_with_freshness(image_path, class_mapping)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        img_batch = np.expand_dims(img_f, axis=0)\n",
    "        stats_batch = np.expand_dims(stats, axis=0)\n",
    "        \n",
    "        # Make prediction\n",
    "        class_pred, freshness_pred = model.predict([img_batch, stats_batch], verbose=0)\n",
    "        \n",
    "        # Get results\n",
    "        predicted_class = np.argmax(class_pred[0])\n",
    "        confidence = np.max(class_pred[0])\n",
    "        freshness_score = freshness_pred[0][0]\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': predicted_class,\n",
    "            'predicted_class_name': quality_class_names[predicted_class],\n",
    "            'confidence': float(confidence),\n",
    "            'freshness_score': float(freshness_score),\n",
    "            'color_features': stats.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_uploaded_image(uploaded_file):\n",
    "    \"\"\"Process uploaded image for prediction\"\"\"\n",
    "    # Save uploaded file temporarily\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        temp_path = tmp_file.name\n",
    "    \n",
    "    return temp_path\n",
    "\n",
    "# ===== SECTION 9: STREAMLIT DASHBOARD =====\n",
    "\n",
    "def create_streamlit_dashboard():\n",
    "    \"\"\"Create Streamlit dashboard for live predictions\"\"\"\n",
    "    \n",
    "    st.set_page_config(\n",
    "        page_title=\"Fruits & Vegetables Quality Assessment\",\n",
    "        page_icon=\"ðŸŽ\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    st.title(\"ðŸŽ Fruits & Vegetables Quality Assessment System\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Initialize database\n",
    "    init_database()\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    app_mode = st.sidebar.selectbox(\n",
    "        \"Choose App Mode\",\n",
    "        [\"Live Prediction\", \"Database View\", \"Model Training\"]\n",
    "    )\n",
    "    \n",
    "    if app_mode == \"Live Prediction\":\n",
    "        st.header(\"ðŸ“· Live Quality Prediction\")\n",
    "        \n",
    "        # Image upload\n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Upload an image of fruit or vegetable\", \n",
    "            type=['jpg', 'jpeg', 'png']\n",
    "        )\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            if uploaded_file is not None:\n",
    "                # Display uploaded image\n",
    "                image = Image.open(uploaded_file)\n",
    "                st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "                \n",
    "                # Process and predict\n",
    "                temp_path = process_uploaded_image(uploaded_file)\n",
    "                \n",
    "                # Load model\n",
    "                try:\n",
    "                    model = load_model('improved_produce_quality_model.h5')\n",
    "                    \n",
    "                    with st.spinner('Analyzing image...'):\n",
    "                        result = predict_single_image(model, temp_path)\n",
    "                        \n",
    "                        if result:\n",
    "                            # Display results\n",
    "                            st.success(\"âœ… Analysis Complete!\")\n",
    "                            \n",
    "                            # Quality result\n",
    "                            quality_color = {\n",
    "                                'unripe': 'blue',\n",
    "                                'ripe': 'green', \n",
    "                                'overripe': 'orange',\n",
    "                                'bruised': 'red'\n",
    "                            }\n",
    "                            \n",
    "                            st.subheader(\"ðŸ“Š Prediction Results\")\n",
    "                            \n",
    "                            col1, col2, col3 = st.columns(3)\n",
    "                            \n",
    "                            with col1:\n",
    "                                st.metric(\n",
    "                                    \"Quality\", \n",
    "                                    result['predicted_class_name'].upper(),\n",
    "                                    delta=f\"{result['confidence']:.1%} confidence\"\n",
    "                                )\n",
    "                            \n",
    "                            with col2:\n",
    "                                st.metric(\n",
    "                                    \"Freshness Score\", \n",
    "                                    f\"{result['freshness_score']:.1f}/10.0\"\n",
    "                                )\n",
    "                            \n",
    "                            with col3:\n",
    "                                freshness_status = \"Excellent\" if result['freshness_score'] >= 8.0 else \\\n",
    "                                                \"Good\" if result['freshness_score'] >= 6.0 else \\\n",
    "                                                \"Fair\" if result['freshness_score'] >= 4.0 else \"Poor\"\n",
    "                                st.metric(\"Freshness Status\", freshness_status)\n",
    "                            \n",
    "                            # Save to database\n",
    "                            prediction_id = save_prediction_to_db(\n",
    "                                temp_path,\n",
    "                                result['predicted_class'],\n",
    "                                result['freshness_score'],\n",
    "                                result['confidence'],\n",
    "                                result['color_features']\n",
    "                            )\n",
    "                            \n",
    "                            st.info(f\"âœ… Prediction saved to database (ID: {prediction_id})\")\n",
    "                            \n",
    "                        else:\n",
    "                            st.error(\"âŒ Failed to analyze image\")\n",
    "                    \n",
    "                    # Clean up temp file\n",
    "                    os.unlink(temp_path)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"âŒ Error loading model: {e}\")\n",
    "                    st.info(\"Please train the model first using the 'Model Training' section\")\n",
    "        \n",
    "        with col2:\n",
    "            # Display recent predictions\n",
    "            st.subheader(\"ðŸ“ˆ Recent Predictions\")\n",
    "            recent_predictions = get_recent_predictions(5)\n",
    "            \n",
    "            if recent_predictions:\n",
    "                for pred in recent_predictions:\n",
    "                    pred_id, timestamp, _, pred_class, pred_name, freshness, confidence, _, _ = pred\n",
    "                    \n",
    "                    with st.container():\n",
    "                        col1, col2 = st.columns([2, 1])\n",
    "                        with col1:\n",
    "                            st.write(f\"**{pred_name.upper()}**\")\n",
    "                            st.write(f\"Freshness: {freshness:.1f}/10.0\")\n",
    "                        with col2:\n",
    "                            st.write(f\"Conf: {confidence:.1%}\")\n",
    "                        st.write(f\"Time: {timestamp[:19]}\")\n",
    "                        st.markdown(\"---\")\n",
    "            else:\n",
    "                st.info(\"No predictions yet. Upload an image to get started!\")\n",
    "    \n",
    "    elif app_mode == \"Database View\":\n",
    "        st.header(\"ðŸ“Š Prediction Database\")\n",
    "        \n",
    "        # Statistics\n",
    "        stats = get_prediction_stats()\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Total Predictions\", stats['total_predictions'])\n",
    "        \n",
    "        with col2:\n",
    "            st.metric(\"Average Freshness\", f\"{stats['avg_freshness']:.1f}/10.0\" if stats['avg_freshness'] else \"N/A\")\n",
    "        \n",
    "        with col3:\n",
    "            st.metric(\"Database Status\", \"âœ… Active\")\n",
    "        \n",
    "        # Class distribution chart\n",
    "        st.subheader(\"Class Distribution\")\n",
    "        if stats['class_distribution']:\n",
    "            df_dist = pd.DataFrame(stats['class_distribution'], columns=['Class', 'Count'])\n",
    "            st.bar_chart(df_dist.set_index('Class'))\n",
    "        \n",
    "        # Recent predictions table\n",
    "        st.subheader(\"All Predictions\")\n",
    "        all_predictions = get_recent_predictions(100)  # Get last 100 predictions\n",
    "        \n",
    "        if all_predictions:\n",
    "            df = pd.DataFrame(all_predictions, columns=[\n",
    "                'ID', 'Timestamp', 'Image Path', 'Class', 'Class Name', \n",
    "                'Freshness', 'Confidence', 'Features', 'Processed Path'\n",
    "            ])\n",
    "            \n",
    "            # Display relevant columns\n",
    "            display_df = df[['Timestamp', 'Class Name', 'Freshness', 'Confidence']]\n",
    "            display_df['Timestamp'] = pd.to_datetime(display_df['Timestamp'])\n",
    "            display_df = display_df.sort_values('Timestamp', ascending=False)\n",
    "            \n",
    "            st.dataframe(display_df, use_container_width=True)\n",
    "            \n",
    "            # Export option\n",
    "            if st.button(\"Export to CSV\"):\n",
    "                csv = df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"Download CSV\",\n",
    "                    data=csv,\n",
    "                    file_name=\"predictions_export.csv\",\n",
    "                    mime=\"text/csv\"\n",
    "                )\n",
    "        else:\n",
    "            st.info(\"No predictions in database yet.\")\n",
    "    \n",
    "    elif app_mode == \"Model Training\":\n",
    "        st.header(\"ðŸ¤– Model Training\")\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        This section allows you to train the quality assessment model on your dataset.\n",
    "        Make sure your dataset is properly organized before training.\n",
    "        \"\"\")\n",
    "        \n",
    "        if st.button(\"ðŸš€ Start Model Training\"):\n",
    "            with st.spinner('Training model... This may take several minutes.'):\n",
    "                try:\n",
    "                    model, history, results = train_and_evaluate_model()\n",
    "                    \n",
    "                    if model is not None:\n",
    "                        st.success(\"âœ… Model training completed successfully!\")\n",
    "                        \n",
    "                        col1, col2 = st.columns(2)\n",
    "                        \n",
    "                        with col1:\n",
    "                            st.metric(\"Final Accuracy\", f\"{results[3]:.2%}\")\n",
    "                            st.metric(\"Training Loss\", f\"{results[1]:.4f}\")\n",
    "                        \n",
    "                        with col2:\n",
    "                            st.metric(\"Freshness MAE\", f\"{results[5]:.4f}\")\n",
    "                            st.metric(\"Validation Loss\", f\"{results[2]:.4f}\")\n",
    "                        \n",
    "                        # Save performance to database\n",
    "                        conn = sqlite3.connect('produce_quality.db')\n",
    "                        cursor = conn.cursor()\n",
    "                        cursor.execute('''\n",
    "                            INSERT INTO model_performance \n",
    "                            (timestamp, accuracy, freshness_mae, training_loss, validation_loss)\n",
    "                            VALUES (?, ?, ?, ?, ?)\n",
    "                        ''', (datetime.now().isoformat(), results[3], results[5], results[1], results[2]))\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                        \n",
    "                    else:\n",
    "                        st.error(\"âŒ Model training failed. Check the console for details.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    st.error(f\"âŒ Training error: {e}\")\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"**Fruits & Vegetables Quality Assessment System** â€¢ \"\n",
    "        \"Built with TensorFlow & Streamlit\"\n",
    "    )\n",
    "\n",
    "# ===== SECTION 10: MAIN EXECUTION =====\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=== FRUITS & VEGETABLES QUALITY ASSESSMENT SYSTEM ===\\n\")\n",
    "    print(f\"ðŸ“ Dataset location: {BASE_DIR}\\n\")\n",
    "    \n",
    "    # Initialize database\n",
    "    init_database()\n",
    "    \n",
    "    # First check dataset structure\n",
    "    print(\"Step 0: Checking dataset structure...\")\n",
    "    if not check_dataset_structure():\n",
    "        print(\"âŒ Dataset structure issues found. Please check your folder organization.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStep 1: Setting up training environment...\")\n",
    "    \n",
    "    # Check if model exists, if not offer to train\n",
    "    if not os.path.exists('improved_produce_quality_model.h5'):\n",
    "        print(\"ðŸ¤– No trained model found. Starting training...\")\n",
    "        model, history, results = train_and_evaluate_model()\n",
    "    else:\n",
    "        print(\"âœ… Trained model found. You can use the Streamlit dashboard for predictions.\")\n",
    "    \n",
    "    # Launch Streamlit dashboard\n",
    "    print(\"\\nðŸŽ¯ Starting Streamlit Dashboard...\")\n",
    "    print(\"ðŸ‘‰ Run this command in your terminal to start the dashboard:\")\n",
    "    print(\"   streamlit run your_script_name.py\")\n",
    "    \n",
    "    # For direct execution within this script\n",
    "    create_streamlit_dashboard()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can run either:\n",
    "    # 1. main() for training and setup\n",
    "    # 2. create_streamlit_dashboard() for just the dashboard\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
